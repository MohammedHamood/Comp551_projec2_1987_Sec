{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitmyrootcondad860efe0ffde43f19424cf00c4ac03a3",
   "display_name": "Python 3.7.4 64-bit ('my_root': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Clyde\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Clyde\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Clyde\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nDownloading Dataset ...\nDataset Downloaded\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pipelines_FEngineering as BaseLine\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import preprocessingNLP as PNLP\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "\"\"\"\n",
    "loading test and training data: IMDB Reviews\n",
    "\"\"\"\n",
    "print(\"Downloading Dataset ...\")\n",
    "IMDB_train = load_files('C:/Users/Clyde/Downloads/aclImdb_v1/aclImdb/train/',\n",
    "                        categories=(\"pos\", \"neg\"), encoding='utf-8')\n",
    "IMDB_test = load_files('C:/Users/Clyde/Downloads/aclImdb_v1/aclImdb/test/',\n",
    "                        categories=(\"pos\", \"neg\"), encoding='utf-8')\n",
    "\n",
    "print(\"Dataset Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PREPROCESSING ...\nPREPROCESSING DONE!\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocessing\n",
    "\"\"\"\n",
    "print(\"PREPROCESSING ...\")\n",
    "IMDB_train.data = PNLP.customNLP(IMDB_train.data)\n",
    "\n",
    "IMDB_test.data = PNLP.customNLP(IMDB_test.data)\n",
    "\n",
    "IMDB_train.data, IMDB_train.target = PNLP.removeEmptyInstances(IMDB_train.data, IMDB_train.target)\n",
    "\n",
    "print(\"PREPROCESSING DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using randomized search we have a number of parameters that will be used as neighbourhoods for performing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nworks\n0.73824\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=12, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=2,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.73268\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int32' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-50215497a7c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT_base_pridect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMDB_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    426\u001b[0m                              % self.best_estimator_)\n\u001b[0;32m    427\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[0mscore_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my_root\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int32' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pipelines_FEngineering as BaseLine\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__criterion': ('gini', 'entropy'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    'clf__max_features': (None, 'auto', 'sqrt', 'log2'),\n",
    "    'clf__max_leaf_nodes': (None, 20, 30, 40, 80),\n",
    "}\n",
    "\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(),\n",
    "                                               search=\"random\")\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie test\n",
    "\"\"\"\n",
    "\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.74128\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=12, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=2,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.73576\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__min_samples_split': (100),\n",
    "    # 'clf__min_samples_leaf': (2),\n",
    "    # 'clf__max_features': (None),\n",
    "    # 'clf__max_leaf_nodes': ( 40),\n",
    "}\n",
    "\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini', max_depth=12, min_samples_split=100, min_samples_leaf=2, max_features=None, max_leaf_nodes=40),search=\"grid\")\n",
    "\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find best max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.75252\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=None, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=2,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.74656\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (100),\n",
    "    # 'clf__min_samples_leaf': (2),\n",
    "    # 'clf__max_features': (None),\n",
    "    # 'clf__max_leaf_nodes': ( 40),\n",
    "}\n",
    "\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini', min_samples_split=100, min_samples_leaf=2, max_features=None, max_leaf_nodes=40),search=\"grid\")\n",
    "\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find best min sample leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.75252\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=None, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=50,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.74704\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (100),\n",
    "    'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__max_features': (None),\n",
    "    # 'clf__max_leaf_nodes': ( 40),\n",
    "}\n",
    "\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_split=100, min_samples_leaf=2, max_features=None, max_leaf_nodes=40),search=\"grid\")\n",
    "\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.75252\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=None, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=50,\n                                        min_samples_split=50,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.74704\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    #'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__max_features': (None),\n",
    "    # 'clf__max_leaf_nodes': ( 40),\n",
    "}\n",
    "\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_leaf=50, max_features=None, max_leaf_nodes=40),search=\"grid\")\n",
    "\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Runtime: 625.0683965682983 seconds\n0.75252\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=None, max_features=None,\n                                        max_leaf_nodes=40,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=50,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.74704\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    'clf__max_features': (None, 'auto', 'sqrt', 'log2'),\n",
    "    # 'clf__max_leaf_nodes': ( 40),\n",
    "}\n",
    "start_time = time.time()\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_split=100, min_samples_leaf=50, max_leaf_nodes=40),search=\"grid\")\n",
    "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Runtime: 940.9532432556152 seconds\n0.75796\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n                                        max_depth=None, max_features=None,\n                                        max_leaf_nodes=80,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=50,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort=False, random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.74792\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    # 'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__max_features': (None, 'auto', 'sqrt', 'log2'),\n",
    "    'clf__max_leaf_nodes': (None, 20, 30, 40, 80),\n",
    "\n",
    "}\n",
    "start_time = time.time()\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_split=100, max_features=None, min_samples_leaf=50),search=\"grid\")\n",
    "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nRuntime: 1041.2813611030579 seconds\n0.75796\nPipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=200000, min_df=1,\n                                 ngram_range=(1, 3), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=N...\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 DecisionTreeClassifier(ccp_alpha=0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=80,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=50,\n                                        min_samples_split=100,\n                                        min_weight_fraction_leaf=0.0,\n                                        presort='deprecated', random_state=None,\n                                        splitter='best'))],\n         verbose=False)\n0.7479199999999999\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__max_features': (None, 'auto', 'sqrt', 'log2'),\n",
    "    #'clf__max_leaf_nodes': (None, 20, 30, 40, 80),\n",
    "\n",
    "}\n",
    "start_time = time.time()\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_split=100, max_features=None, min_samples_leaf=50, max_leaf_nodes=80),search=\"grid\")\n",
    "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    " DecisionTreeClassifie train\n",
    "\"\"\"\n",
    "parameters = {\n",
    "    # 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 1000,5000, 10000, 50000),\n",
    "    # 'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__criterion': ('gini'),\n",
    "    'clf__ccp_alpha': (0, .05, .1, .2, .3, .4),\n",
    "    # 'clf__max_depth': (None, 5, 7, 8, 10, 12, 20, 100),\n",
    "    # 'clf__min_samples_split': (2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__min_samples_leaf': (1, 2, 3, 4, 8, 10, 50, 100),\n",
    "    # 'clf__max_features': (None, 'auto', 'sqrt', 'log2'),\n",
    "    #'clf__max_leaf_nodes': (None, 20, 30, 40, 80),\n",
    "\n",
    "}\n",
    "start_time = time.time()\n",
    "DT_base = BaseLine.Pipeline_FeatureEngineering(IMDB_train.data, IMDB_train.target,\n",
    "                                               parameters=parameters, CV=10,\n",
    "                                               reductionMethod=TruncatedSVD(n_components=100),\n",
    "                                               reductionType=3, model=DecisionTreeClassifier(criterion='gini',max_depth=None, min_samples_split=100, max_features=None, min_samples_leaf=50, max_leaf_nodes=80),search=\"grid\")\n",
    "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
    "DT_base_pridect = DT_base.predict(IMDB_test.data)\n",
    "print(np.mean(DT_base_pridect == IMDB_test.target))\n",
    "print(DT_base.best_estimator_)\n",
    "print(DT_base.best_score_)"
   ]
  }
 ]
}